{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face_Recognition_DLIB_Image.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MqtU9q3_4sJ4",
        "colab_type": "code",
        "outputId": "ad97edd0-7a40-4e62-feb6-3c3d56b48b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pdhruv93/YOLO-Face-Recognition-DLIB.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'YOLO-Face-Recognition-DLIB' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f0uI2eGc4zE4",
        "colab_type": "code",
        "outputId": "3e9f903a-6bb5-4fcb-d050-a4c3918dace2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "%cd YOLO-Face-Recognition-DLIB//opencv-face-recognition-dlib"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/YOLO-Face-Recognition-DLIB/opencv-face-recognition-dlib/YOLO-Face-Recognition-DLIB/opencv-face-recognition-dlib/YOLO-Face-Recognition-DLIB/opencv-face-recognition-dlib/YOLO-Face-Recognition-DLIB/opencv-face-recognition-dlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n9cwIvA45ad0",
        "colab_type": "code",
        "outputId": "440d8273-e291-458e-ab3f-d51017465beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install face_recognition"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: face_recognition in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.16.0)\n",
            "Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (0.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (4.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.14.6)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->face_recognition) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V_pMFkLv4K6N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from imutils import paths\n",
        "import face_recognition\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nB14gEjR9b89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datasetPath=\"dataset//dhruv//\"\n",
        "encodingsPath=\"encodings.pickle\"\n",
        "# command line arguments in dict form\n",
        "#dataset-- path to our dataset(input images)\n",
        "#encodings -- path to our face encodings .pickle file\n",
        "#detection-method--cnn/hog..The CNN method is more accurate but slower. HOG is faster but less accurate.\n",
        "args={'dataset': datasetPath , 'encodings': encodingsPath , 'detection_method': \"hog\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MRxSmUc3_v9E",
        "colab_type": "code",
        "outputId": "52a720a8-49cc-463b-d08e-09bc029f9371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# grab the paths to the input images in our dataset\n",
        "print(\"[INFO] quantifying faces...\")\n",
        "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
        "print(imagePaths)\n",
        " \n",
        "# These two lists will contain the face encodings and corresponding names for each person in the dataset(currently only 1 person)\n",
        "knownEncodings = []\n",
        "knownNames = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] quantifying faces...\n",
            "['dataset//dhruv//IMG_5249.JPG', 'dataset//dhruv//IMG_20180605_211534.jpg', 'dataset//dhruv//IMG_20180608_233825.jpg', 'dataset//dhruv//IMG_20181003_221421.jpg', 'dataset//dhruv//IMG_5075.JPG', 'dataset//dhruv//IMG_3759.JPG', 'dataset//dhruv//IMG_20180410_135431.jpg', 'dataset//dhruv//IMG_20181203_132032.jpg', 'dataset//dhruv//IMG_20180526_095231.jpg', 'dataset//dhruv//IMG_20180408_131938.jpg', 'dataset//dhruv//IMG-20180903-WA0121.jpg', 'dataset//dhruv//IMG_3760.JPG', 'dataset//dhruv//IMG_20180520_184325.jpg', 'dataset//dhruv//IMG_20181019_091156.jpg', 'dataset//dhruv//IMG_5055.JPG', 'dataset//dhruv//IMG_20180520_184234.jpg', 'dataset//dhruv//IMG_5248.JPG', 'dataset//dhruv//IMG-20180903-WA0129.jpg', 'dataset//dhruv//IMG_20180605_211515.jpg', 'dataset//dhruv//IMG_20180929_170034.jpg', 'dataset//dhruv//IMG-20170911-WA0000.jpg', 'dataset//dhruv//IMG_20180605_211540.jpg', 'dataset//dhruv//IMG_20180404_234725.jpg', 'dataset//dhruv//IMG_20180601_153933.jpg', 'dataset//dhruv//IMG_20180411_145005.jpg', 'dataset//dhruv//IMG-20180903-WA0120.jpg', 'dataset//dhruv//IMG_4888.JPG', 'dataset//dhruv//IMG_4887.JPG', 'dataset//dhruv//IMG_20181125_190807.jpg', 'dataset//dhruv//IMG_20180520_184209.jpg', 'dataset//dhruv//IMG_20180605_212342.jpg', 'dataset//dhruv//IMG_20180413_190414.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w33V_xAtBHpw",
        "colab_type": "code",
        "outputId": "bc0d15b4-bd8b-4927-91c2-06d75a6b3ff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "# loop over the image paths\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "  print(\"[INFO] processing image {}/{}\".format(i + 1,len(imagePaths)))\n",
        "  \n",
        "  # extract the person name from the image path\n",
        "  # dataset//dhruv//IMG_20180605_211534.jpg.. 0 is dataset 1 is dhruv\n",
        "  name = imagePath.split(\"//\")[1]\n",
        "  \n",
        "  # load the input image and convert it from BGR (OpenCV ordering) to dlib ordering (RGB)\n",
        "  image = cv2.imread(imagePath)\n",
        "  rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "  # weâ€™re going to detect a face and get the (x, y)-coordinates of the bounding boxes corresponding to each face in the input image\n",
        "  #NOTE: If there are multiple faces in an image, the module treats that all are of single person.\n",
        "  boxes = face_recognition.face_locations(rgb,model=args[\"detection_method\"])\n",
        "  \n",
        "  # compute the facial embedding for the face\n",
        "  encodings = face_recognition.face_encodings(rgb, boxes)\n",
        "  \n",
        "  # loop over the encodings\n",
        "  for encoding in encodings:\n",
        "    # add each encoding + name to our set of known names and encodings\n",
        "    knownEncodings.append(encoding)\n",
        "    knownNames.append(name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] processing image 1/32\n",
            "[INFO] processing image 2/32\n",
            "[INFO] processing image 3/32\n",
            "[INFO] processing image 4/32\n",
            "[INFO] processing image 5/32\n",
            "[INFO] processing image 6/32\n",
            "[INFO] processing image 7/32\n",
            "[INFO] processing image 8/32\n",
            "[INFO] processing image 9/32\n",
            "[INFO] processing image 10/32\n",
            "[INFO] processing image 11/32\n",
            "[INFO] processing image 12/32\n",
            "[INFO] processing image 13/32\n",
            "[INFO] processing image 14/32\n",
            "[INFO] processing image 15/32\n",
            "[INFO] processing image 16/32\n",
            "[INFO] processing image 17/32\n",
            "[INFO] processing image 18/32\n",
            "[INFO] processing image 19/32\n",
            "[INFO] processing image 20/32\n",
            "[INFO] processing image 21/32\n",
            "[INFO] processing image 22/32\n",
            "[INFO] processing image 23/32\n",
            "[INFO] processing image 24/32\n",
            "[INFO] processing image 25/32\n",
            "[INFO] processing image 26/32\n",
            "[INFO] processing image 27/32\n",
            "[INFO] processing image 28/32\n",
            "[INFO] processing image 29/32\n",
            "[INFO] processing image 30/32\n",
            "[INFO] processing image 31/32\n",
            "[INFO] processing image 32/32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dE5dqAfInN2z",
        "colab_type": "code",
        "outputId": "7cfe1041-53ad-42a8-86b7-0626f142c53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(knownNames))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KBEYBME6oEj4",
        "colab_type": "code",
        "outputId": "e3b7a5c2-4df6-4bec-9c1f-5b77dc735048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!rm encodings.pickle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset  encodings.pickle  examples  YOLO-Face-Recognition-DLIB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4aJ8IC6oB1Vt",
        "colab_type": "code",
        "outputId": "96187143-a0ac-4039-d995-4d2e02877ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# dump the facial encodings + names to disk\n",
        "print(\"[INFO] serializing encodings...\")\n",
        "data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
        "f = open(args[\"encodings\"], \"wb\")\n",
        "f.write(pickle.dumps(data))\n",
        "f.close()\n",
        "#TILL NOW WE HAVE FOUND THE ENCODINGS OF FACES IN INPUT DATSET"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] serializing encodings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wZfC03AuEzHJ",
        "colab_type": "code",
        "outputId": "a24145c6-7229-4183-b57b-fd90ca921b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset  encodings.pickle  examples  YOLO-Face-Recognition-DLIB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kl1pYGI8KLWw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputImagePath=\"examples//IMG_0962.JPG\"\n",
        "encodingsPath=\"encodings.pickle\"\n",
        "# command line arguments in dict form\n",
        "#image-- path to our image on which we will detect face\n",
        "#encodings -- path to our face encodings .pickle file\n",
        "#detection-method--cnn/hog..The CNN method is more accurate but slower. HOG is faster but less accurate.\n",
        "args={'image': inputImagePath , 'encodings': encodingsPath , 'detection_method': \"hog\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H8IjBUiZLbHw",
        "colab_type": "code",
        "outputId": "f706eafa-1f14-4dc3-eada-84b78abec5d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "# load the known faces and embeddings\n",
        "print(\"[INFO] loading encodings...\")\n",
        "data = pickle.loads(open(args[\"encodings\"], \"rb\").read())\n",
        " \n",
        "# load the input image and convert it from BGR to RGB\n",
        "image = cv2.imread(args[\"image\"])\n",
        "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        " \n",
        "# detect the (x, y)-coordinates of the bounding boxes corresponding to each face in the input image, then compute the facial embeddings for each face\n",
        "print(\"[INFO] recognizing faces...\")\n",
        "boxes = face_recognition.face_locations(rgb,model=args[\"detection_method\"])\n",
        "encodings = face_recognition.face_encodings(rgb, boxes)\n",
        "\n",
        "print(\"{} faces detected\".format(len(boxes)))\n",
        " \n",
        "# initialize the list of names for each face detected\n",
        "names = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading encodings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f8afd1949f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load the input image and convert it from BGR to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# detect the (x, y)-coordinates of the bounding boxes corresponding to each face in the input image, then compute the facial embeddings for each face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HjnKCVrIiWn_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loop over the facial embeddings\n",
        "for encoding in encodings:\n",
        "  # attempt to match each face in the input image to our known encodings\n",
        "  #This function returns a list of True / False  values, one for each image in our dataset. \n",
        "  #For our example, there are 32 images in the dataset and therefore the returned list will have 32 boolean values.\n",
        "  matches = face_recognition.compare_faces(data[\"encodings\"],encoding)\n",
        "  name = \"Unknown\"\n",
        "  \n",
        "  # check to see if we have found a match\n",
        "  if True in matches:\n",
        "    #get indexes where this particular face(encoding) was matched in the dataset images\n",
        "    matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
        "    print(\"Face found in {} images\".format(len(matchedIdxs)))\n",
        "    counts = {}\n",
        "    \n",
        "    # loop over the matched indexes and maintain a count for each recognized face\n",
        "    for i in matchedIdxs:\n",
        "      name = data[\"names\"][i]\n",
        "      counts[name] = counts.get(name, 0) + 1\n",
        "    \n",
        "    # determine the current face with the largest number of voted face\n",
        "    name = max(counts, key=counts.get)\n",
        "    \n",
        "  # if there would be no True in matched indexes store name to Unknown\n",
        "  names.append(name)\n",
        "  #now we have got name for each face in our input image\n",
        "print(names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OivP586zOQEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for very box that is detected, there would be a name\n",
        "for ((top, right, bottom, left), name) in zip(boxes, names):\n",
        "  if(name != 'Unknown'):\n",
        "    # draw the predicted face name on the image\n",
        "    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
        "    y = top - 15 if top - 15 > 15 else top + 15\n",
        "    cv2.putText(image, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
        "\n",
        "# show the output image\n",
        "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "plt.imshow(image,shape=(50,50))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}